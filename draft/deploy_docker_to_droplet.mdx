# main idea for this blog
- it should tell the story of someone who are new/don't have experience with docker and deploying to virtual machine doing it in 1 hour with the help of AI
- highlight the power of AI in doing new thing and explore new thing.
- Specify the use case here: experience engineer with aws but deploying simple website in aws has many hidden cost. even simple ec2 deployment would have hidden cost of bandwidth, data. Before switching has high cost of get to know new platform, but now with AI it become so much easier
# detailed steps to do this
- goal: set up a technical blog but with some flexibility to do SEO so want to self-host website instead of using build in blog platform like substack. at the same time want to have low and fixed cost. tech stack for the website choose next.js because of familiality and SSR. deployment was the question to satisfy the low and fixed cost requirement.
- 1st step: do research with chatgpt. answer the basic question of what available options. Getting list of them: vercel, netlify, cloudflare pages, digital ocean droplet, github pages, aws amplify or ec2. each option actually has different use cases so need to really read about them and launch separate research about each of them if needed. also there will be some new concept/information about hosting that i may not even hear of before, keep an open mind and research whatever you feel don't understand. before this research task may take too long and prevent the "bias for action" to get project done so I may skip 1 or 2 thing, but with AI i went much deeper than before and get better understanding to choose the right option. remember AI suggestion not always the correct one if you don't give it the right requirement, and you can only have the right requirement if you really understand your problem. at the end i chose digital ocean droplet because of its cheap + predictable cost. i understand the trade off is new learning curve for docker deployment, only vertical scaling for simple solution, and to scale horizontally will need complicated harness (kurbenetes to deploy to multiple machine plus load balancer and database layer separately). but understand about the requirement for this project make me very confident in this choice. only problem is to learn to deploy with docker as i haven't done it in 5-10 years
- 2nd step: bundle and deploy the next.js to docker. note that before this i have already build a next.js project for technical blog. with the help of context engineering practice i have been able to one-shot the main functionality of this project with claude code, then doing an autonomous debugging session with cursor to fix js/ui bugs (cursor control headless browser to auto test, see visual bugs and fix). it quite marvelous the development process for that, but i will save the detail for a separate blog about context engineering with more example and practice. after i got the next.js project, next thing is to deploy it to the droplet. i know next to nothing about this, so the first thing that i do is to launch another chatgpt session to learn the basic about this. key point here is to start small with the essential, then keep asking question about important things until you can build your own mental model about how to do this. once i build my mental model about the steps needed to deploy, i start doing it: register with digital ocean, getting a droplet, set up ssh to get in and prepare the application needed inside the droplet. have to say that interaction with the virtual machine is another challenge because it's different os (linux) and shell (bash), so most of the command i just do ask chatgpt/claude code tell me. another thing i realize that terminal/shell done so well compare to the current AI coding is that it display very clear result. i can run command that i don't fully understand but as long as it doesn't return error/red message i don't really need to care. after setting up the droplet i git clone the project to it. i already ask claude code to build the next.js with docker build script so then i can easily build and run docker inside the droplet. next i need to set up reverse proxy and there were option between nginx + certbot vs caddy. i did another chatgpt session to compare and chose caddy for its simplicity. lastly change my domain and create a subdomain to point to my next.js in the droplet.
# lessons learn
- 3 big lessons i have after this process
- first lesson: this is the new age of try anything and do anything. with ai each person should remeasure their capacity and explore more new frontier than they were before. the cost is much lower, the risk is much lower, the only barrier is your will to learn and try new thing
- second lesson: curiosity and learning is a must. you need to rewire your brain, change your practice. willing to do more digging, more researching. again because the cost is low, but there are also more information so you need to learn to read and verify them carefully. example in this blog is how i research about hosting, reverse proxy, docker practice, etc. need to put in more work upfront before throwing the requirement to AI, and keep changing assumption/find better decision if new information emerge
- 3rd lesson: new paradigm of programming. we need to have better unit test, better result and error message. with ai it's much easier to build in those. build more script to run. give more harness to our software. make it as safe and deterministic as a bash/shell command. that's the way to control the quality of ai generated code.